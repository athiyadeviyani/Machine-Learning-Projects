{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MulticlassClassificationOnStackOverflowQuestions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PhmqkiX5Hhv",
        "colab_type": "text"
      },
      "source": [
        "# Multiclass Classification on Stack Overflow Questions\n",
        "\n",
        "Train a multiclass classifier to predict the tag of a programming question on Stack Overflow.\n",
        "\n",
        "The dataset contains the body of several thousand programming questions (for example, \"How can sort a dictionary by value in Python?\") posted to Stack Overflow. Each of these is labeled with exactly one tag (either Python, CSharp, JavaScript, or Java). The task is to take a question as input, and predict the appropriate tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDuZAIYz5bBI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "612658ee-db0b-4d89-cd94-c7b64e124b6a"
      },
      "source": [
        "!pip install tf-nightly"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/4e/5d6d144e2733acaa3ef451345d74bfb1aa28884e7086dff8b4e37b4ec7f0/tf_nightly-2.4.0.dev20200922-cp36-cp36m-manylinux2010_x86_64.whl (390.7MB)\n",
            "\u001b[K     |████████████████████████████████| 390.7MB 41kB/s \n",
            "\u001b[?25hCollecting flatbuffers>=1.12\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/26/712e578c5f14e26ae3314c39a1bdc4eb2ec2f4ddc89b708cf8e0a0d20423/flatbuffers-1.12-py2.py3-none-any.whl\n",
            "Collecting tf-estimator-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/54/e6255de0770a055ed3b9bfc90b254deb6cece5621fcffa0d0300199927c5/tf_estimator_nightly-2.4.0.dev2020091801-py2.py3-none-any.whl (460kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 50.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.15.0)\n",
            "Collecting tb-nightly<3.0.0a0,>=2.4.0a0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/a3/32f95366a8390d73cad81401f1e0b5e6408464c45e0581ed4f9d5ebd73e3/tb_nightly-2.4.0a20200921-py3-none-any.whl (10.2MB)\n",
            "\u001b[K     |████████████████████████████████| 10.2MB 47.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.32.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.35.1)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.18.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.7.4.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (50.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.17.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.7.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (4.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.1.0)\n",
            "Installing collected packages: flatbuffers, tf-estimator-nightly, tb-nightly, tf-nightly\n",
            "Successfully installed flatbuffers-1.12 tb-nightly-2.4.0a20200921 tf-estimator-nightly-2.4.0.dev2020091801 tf-nightly-2.4.0.dev20200922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw4vfuT65k0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A8_3apZ7T8V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0475d73-008e-4772-c83b-0b82d7bcca4a"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.0-dev20200922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6nLREEp7Vif",
        "colab_type": "text"
      },
      "source": [
        "## Download the BigQuery dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10mvHYA07XrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"http://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ufnHSOy7aVh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "2a870fa6-4d13-4813-b567-aeff019a1ae2"
      },
      "source": [
        "tf.keras.utils.get_file(\"stack_overflow_16k.tar.gz\",\n",
        "                        url,\n",
        "                        untar=True,\n",
        "                        cache_dir='.',\n",
        "                        cache_subdir='')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\n",
            "6053888/6053168 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./stack_overflow_16k.tar.gz'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6Rq1ql67jqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwAX3eGH8Ckm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3ca89e0f-5fe1-426f-8528-09dd5dd1d8a4"
      },
      "source": [
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'train', \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='training', \n",
        "    seed=42\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 files belonging to 4 classes.\n",
            "Using 6400 files for training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5IIiiCt8E8d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9456ee0f-1a3e-4713-f95f-738cd83ab330"
      },
      "source": [
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'train', \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='validation', \n",
        "    seed=42\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 files belonging to 4 classes.\n",
            "Using 1600 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqDgvz8V8Jyi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "244d06e1-b5dd-419d-9d59-f120f4e56d2e"
      },
      "source": [
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'train', \n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 files belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "espnfNcN8MIE",
        "colab_type": "text"
      },
      "source": [
        "## Explore the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-98QlArAAR_w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd586375-efa3-424a-d82d-fdbbaa8d6f48"
      },
      "source": [
        "print(raw_train_ds.class_names)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['csharp', 'java', 'javascript', 'python']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYnJkxHHATS-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "c95445d4-ae5a-4e05-bddb-2e680dba7834"
      },
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(5):\n",
        "    print(text_batch.numpy()[i])\n",
        "    print(label_batch.numpy()[i])\n",
        "    print()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'\"unit testing of setters and getters teacher wanted us to do a comprehensive unit test. for me, this will be the first time that i use junit. i am confused about testing set and get methods. do you think should i test them? if the answer is yes; is this code enough for testing?..  public void testsetandget(){.    int a = 10;.    class firstclass = new class();.    firstclass.setvalue(10);.    int value = firstclass.getvalue();.    assert.asserttrue(\"\"error\"\", value==a);.  }...in my code, i think if there is an error, we can\\'t know that the error is deriving because of setter or getter.\"\\n'\n",
            "1\n",
            "\n",
            "b'\"static class, static constructors and static properties i have a static class that has only static properties and a static constructor. when i try to access or set the value of property (with a backing field) the static constructor is not called. however, if i define a static method and try to call it the constructor is executed...i believe properties are just syntactical sugar and are internally translated as methods. so why does the runtime treats them differently? my class define is given below:..edit: i have removed the code where i was initializing the value of _fileencodingtext inline...edit: the constructor is called but the property is not set. this is probably because \"\"a static constructor runs exactly zero or one times, and runs before a static method call or instance creation in its type\"\". igor ostrovsky and eric lippert have explained it in their blogs...http://blogs.msdn.com/b/pfxteam/archive/2011/05/03/10159682.aspx.http://ericlippert.com/2013/01/31/the-no-lock-deadlock/..internal static class appsettings.{.    static appsettings().    {.        fileencodingtext = \"\"utf8\"\";.    }..private static string _fileencodingtext;.public static string fileencodingtext.{.    get { return _fileencodingtext; }.    set.    {.        string oldvalue = _fileencodingtext;.        _fileencodingtext = value;..        try.        {.            fileencoding = encoding.getencoding(value);.        }.        catch (system.exception).        {.            _fileencodingtext = oldvalue;.            fileencoding = encoding.utf8;.        }.    }.}..public static encoding fileencoding { get; private set; }...}\"\\n'\n",
            "0\n",
            "\n",
            "b'\"is there anyway for switch case statement to do count only the value less than assigned? sorry if the title is misleading, but i don\\'t know how to put my question in words. take a look at the code below. when i run it, if i enter any value between 100 and 109, it will show me \"\"hd-10\"\", but i only want mark == 100 to get hd-10, and anything above 100 will show invalid. any tips?..ps i did import scanner from blank.util, just don\\'t know why it\\'s not showing up in the code below...thanks in advance for the help!..public class gradecase_testresults.{.    public static void main(string [] args).    {.        float total;.        int mark;.        scanner sc = new scanner(system.in);..        system.out.println(\"\"please enter the student\\'s mark\"\");.        total = sc.nextint ();..        mark = (int)total / 10;..        switch(mark).        {.            case 1: case 2:case 3:case 4:.                system.out.println(\"\"f-\"\" + mark);.                break;.            case 5:.                system.out.println(\"\"p-5\"\");.                break;.            case 6:.                system.out.println(\"\"c-6\"\");.                break;.            case 7:.                system.out.println(\"\"d-7\"\");.                break;.            case 8:.                system.out.println(\"\"hd-8\"\");.                break;.            case 9:.                system.out.println(\"\"hd-9\"\");.                break;.            case 10: .                system.out.println(\"\"hd-10\"\");.                break;.            default:.                system.out.println(\"\"invalid\"\");.                break;.        }.    }.}\"\\n'\n",
            "1\n",
            "\n",
            "b'\"is there a way to check a variable that exists in a different script than the original one? i\\'m trying to check if a variable, which was previously set to true in 2.py in 1.py, as 1.py is only supposed to continue if the variable is true...2.py..import os..completed = false..#some stuff here..completed = true...1.py..import 2 ..if completed == true.   #do things...however i get a syntax error at ..if completed == true\"\\n'\n",
            "3\n",
            "\n",
            "b'\"what is the keyword super doing in this blank method? so there\\'s no class extension as in this question, so i\\'m trying to understand what the super() method call is doing?  ..i could understand super if the keyword extend was being used as in the referenced so question, but it is not... public class solver {.   private node result;..   private class node implements comparable&lt;node&gt; {.     node prev;.     board value;.     int moves = 0;.     int priority;.     public node(board value, node previous) {.       super();.       //stuff.       //stuff.     }..     @override.     public string tostring() {.       //stuff.     }..     @override.     public int compareto(node node) {.        //stuff.     }.   }. }\"\\n'\n",
            "1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LCrPNQKAcQC",
        "colab_type": "text"
      },
      "source": [
        "## Prepare data for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_oG8ZebA80i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_features = 5000\n",
        "embedding_dim = 128\n",
        "sequence_length = 500\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRYQlJplBQ1g",
        "colab_type": "text"
      },
      "source": [
        "Make a text-only dataset (no labels) and call adapt:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj1H9bfqBU5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usrdJ1gRBapl",
        "colab_type": "text"
      },
      "source": [
        "## Vectorize the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHZg7KuIBsd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bosPamKSB1D0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJfTgk_PB83-",
        "colab_type": "text"
      },
      "source": [
        "## Configure the dataset for performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m0G7urOCGKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtIh46zGCPfx",
        "colab_type": "text"
      },
      "source": [
        "## Build the model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz2QHCDGCUrp",
        "colab_type": "text"
      },
      "source": [
        "The last layer of the model has to be Dense(4), as there are now four output classes:  Python, CSharp, JavaScript, and Java."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSd2m3LXCdR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(max_features + 1, embedding_dim),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(4)\n",
        "])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMRodLP3CsJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xe_K6naCzDj",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZArq6GifDM2A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "0b2c325b-30e9-427d-af79-cf0021b3d41a"
      },
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5\n",
        ")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "200/200 [==============================] - 10s 46ms/step - loss: 1.3803 - accuracy: 0.2951 - val_loss: 1.3549 - val_accuracy: 0.3850\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 1.3359 - accuracy: 0.4369 - val_loss: 1.2798 - val_accuracy: 0.5875\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 1.2498 - accuracy: 0.5386 - val_loss: 1.1660 - val_accuracy: 0.6525\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 7s 37ms/step - loss: 1.1343 - accuracy: 0.6301 - val_loss: 1.0478 - val_accuracy: 0.7100\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 1.0191 - accuracy: 0.6848 - val_loss: 0.9442 - val_accuracy: 0.7394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz8YJlwhDT4Y",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp47acYvDZs6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "41baf01f-6ec7-47b0-dc36-9b43770df5ae"
      },
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "250/250 [==============================] - 4s 16ms/step - loss: 0.9314 - accuracy: 0.7439\n",
            "Loss:  0.9314426183700562\n",
            "Accuracy:  0.7438750267028809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj891F4ODevo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}